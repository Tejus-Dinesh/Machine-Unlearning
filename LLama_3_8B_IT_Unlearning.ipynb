{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MC902erWj3Vc",
        "outputId": "a0834e8a-e599-4b65-e4e6-26163b3aee03"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting sae_lens\n",
            "  Downloading sae_lens-5.9.1-py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting transformer_lens\n",
            "  Downloading transformer_lens-2.15.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.11/dist-packages (1.9.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Collecting automated-interpretability<1.0.0,>=0.0.5 (from sae_lens)\n",
            "  Downloading automated_interpretability-0.0.8-py3-none-any.whl.metadata (822 bytes)\n",
            "Collecting babe<0.0.8,>=0.0.7 (from sae_lens)\n",
            "  Downloading babe-0.0.7-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.21.0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: matplotlib-inline<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from sae_lens) (0.1.7)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.11/dist-packages (from sae_lens) (3.9.1)\n",
            "Requirement already satisfied: plotly<6.0.0,>=5.19.0 in /usr/local/lib/python3.11/dist-packages (from sae_lens) (5.24.1)\n",
            "Collecting plotly-express<0.5.0,>=0.4.1 (from sae_lens)\n",
            "  Downloading plotly_express-0.4.1-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting pytest-profiling<2.0.0,>=1.7.0 (from sae_lens)\n",
            "  Downloading pytest_profiling-1.8.1-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting python-dotenv<2.0.0,>=1.0.1 (from sae_lens)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting pyzmq==26.0.0 (from sae_lens)\n",
            "  Downloading pyzmq-26.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.2 kB)\n",
            "Collecting safetensors>=0.4.3 (from transformers)\n",
            "  Downloading safetensors-0.4.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: simple-parsing<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from sae_lens) (0.1.7)\n",
            "Collecting typer<0.13.0,>=0.12.3 (from sae_lens)\n",
            "  Downloading typer-0.12.5-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from sae_lens) (4.13.2)\n",
            "Collecting zstandard<0.23.0,>=0.22.0 (from sae_lens)\n",
            "  Downloading zstandard-0.22.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.9 kB)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.18-py311-none-any.whl.metadata (7.5 kB)\n",
            "Collecting fsspec<=2024.6.1,>=2023.1.0 (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.6.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: accelerate>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from transformer_lens) (1.5.2)\n",
            "Collecting beartype<0.15.0,>=0.14.1 (from transformer_lens)\n",
            "  Downloading beartype-0.14.1-py3-none-any.whl.metadata (28 kB)\n",
            "Collecting better-abc<0.0.4,>=0.0.3 (from transformer_lens)\n",
            "  Downloading better_abc-0.0.3-py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: einops>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from transformer_lens) (0.8.1)\n",
            "Collecting fancy-einsum>=0.0.3 (from transformer_lens)\n",
            "  Downloading fancy_einsum-0.0.3-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting jaxtyping>=0.2.11 (from transformer_lens)\n",
            "  Downloading jaxtyping-0.3.1-py3-none-any.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: rich>=12.6.0 in /usr/local/lib/python3.11/dist-packages (from transformer_lens) (13.9.4)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from transformer_lens) (0.2.0)\n",
            "Collecting transformers-stream-generator<0.0.6,>=0.0.5 (from transformer_lens)\n",
            "  Downloading transformers-stream-generator-0.0.5.tar.gz (13 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typeguard<5.0,>=4.2 in /usr/local/lib/python3.11/dist-packages (from transformer_lens) (4.4.2)\n",
            "Requirement already satisfied: wandb>=0.13.5 in /usr/local/lib/python3.11/dist-packages (from transformer_lens) (0.19.9)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.23.0->transformer_lens) (5.9.5)\n",
            "Collecting blobfile<3.0.0,>=2.1.1 (from automated-interpretability<1.0.0,>=0.0.5->sae_lens)\n",
            "  Downloading blobfile-2.1.1-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting boostedblob<0.16.0,>=0.15.3 (from automated-interpretability<1.0.0,>=0.0.5->sae_lens)\n",
            "  Downloading boostedblob-0.15.6-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting httpx<0.28.0,>=0.27.0 (from automated-interpretability<1.0.0,>=0.0.5->sae_lens)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting numpy\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: orjson<4.0.0,>=3.10.1 in /usr/local/lib/python3.11/dist-packages (from automated-interpretability<1.0.0,>=0.0.5->sae_lens) (3.10.16)\n",
            "Requirement already satisfied: scikit-learn<2.0.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from automated-interpretability<1.0.0,>=0.0.5->sae_lens) (1.6.1)\n",
            "Collecting tiktoken>=0.6.0 (from automated-interpretability<1.0.0,>=0.0.5->sae_lens)\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting py2store (from babe<0.0.8,>=0.0.7->sae_lens)\n",
            "  Downloading py2store-0.1.20.tar.gz (143 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.1/143.1 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting graze (from babe<0.0.8,>=0.0.7->sae_lens)\n",
            "  Downloading graze-0.1.29-py3-none-any.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.19.0)\n",
            "Collecting wadler-lindig>=0.1.3 (from jaxtyping>=0.2.11->transformer_lens)\n",
            "  Downloading wadler_lindig-0.1.5-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: traitlets in /usr/local/lib/python3.11/dist-packages (from matplotlib-inline<0.2.0,>=0.1.6->sae_lens) (5.7.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk<4.0.0,>=3.8.1->sae_lens) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk<4.0.0,>=3.8.1->sae_lens) (1.4.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly<6.0.0,>=5.19.0->sae_lens) (9.1.2)\n",
            "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from plotly-express<0.5.0,>=0.4.1->sae_lens) (0.14.4)\n",
            "Requirement already satisfied: scipy>=0.18 in /usr/local/lib/python3.11/dist-packages (from plotly-express<0.5.0,>=0.4.1->sae_lens) (1.14.1)\n",
            "Requirement already satisfied: patsy>=0.5 in /usr/local/lib/python3.11/dist-packages (from plotly-express<0.5.0,>=0.4.1->sae_lens) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from pytest-profiling<2.0.0,>=1.7.0->sae_lens) (1.17.0)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.11/dist-packages (from pytest-profiling<2.0.0,>=1.7.0->sae_lens) (8.3.5)\n",
            "Collecting gprof2dot (from pytest-profiling<2.0.0,>=1.7.0->sae_lens)\n",
            "  Downloading gprof2dot-2025.4.14-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=12.6.0->transformer_lens) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=12.6.0->transformer_lens) (2.18.0)\n",
            "Requirement already satisfied: docstring-parser<1.0,>=0.15 in /usr/local/lib/python3.11/dist-packages (from simple-parsing<0.2.0,>=0.1.6->sae_lens) (0.16)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<0.13.0,>=0.12.3->sae_lens) (1.5.4)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.13.5->transformer_lens) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.13.5->transformer_lens) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb>=0.13.5->transformer_lens) (4.3.7)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.13.5->transformer_lens) (5.29.4)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.13.5->transformer_lens) (2.11.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.13.5->transformer_lens) (2.26.1)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb>=0.13.5->transformer_lens) (1.3.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb>=0.13.5->transformer_lens) (75.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.17-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting pycryptodomex~=3.8 (from blobfile<3.0.0,>=2.1.1->automated-interpretability<1.0.0,>=0.0.5->sae_lens)\n",
            "  Downloading pycryptodomex-3.22.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Collecting lxml~=4.9 (from blobfile<3.0.0,>=2.1.1->automated-interpretability<1.0.0,>=0.0.5->sae_lens)\n",
            "  Downloading lxml-4.9.4-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.7 kB)\n",
            "Collecting uvloop>=0.16.0 (from boostedblob<0.16.0,>=0.15.3->automated-interpretability<1.0.0,>=0.0.5->sae_lens)\n",
            "  Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens) (4.0.12)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<0.28.0,>=0.27.0->automated-interpretability<1.0.0,>=0.0.5->sae_lens) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<0.28.0,>=0.27.0->automated-interpretability<1.0.0,>=0.0.5->sae_lens) (1.0.8)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from httpx<0.28.0,>=0.27.0->automated-interpretability<1.0.0,>=0.0.5->sae_lens) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<0.28.0,>=0.27.0->automated-interpretability<1.0.0,>=0.0.5->sae_lens) (0.14.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=12.6.0->transformer_lens) (0.1.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb>=0.13.5->transformer_lens) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb>=0.13.5->transformer_lens) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb>=0.13.5->transformer_lens) (0.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2.0.0,>=1.2.0->automated-interpretability<1.0.0,>=0.0.5->sae_lens) (3.6.0)\n",
            "Collecting dol (from graze->babe<0.0.8,>=0.0.7->sae_lens)\n",
            "  Downloading dol-0.3.16-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting config2py (from py2store->babe<0.0.8,>=0.0.7->sae_lens)\n",
            "  Downloading config2py-0.1.37-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.11/dist-packages (from py2store->babe<0.0.8,>=0.0.7->sae_lens) (6.5.2)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.11/dist-packages (from pytest->pytest-profiling<2.0.0,>=1.7.0->sae_lens) (2.1.0)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.11/dist-packages (from pytest->pytest-profiling<2.0.0,>=1.7.0->sae_lens) (1.5.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens) (5.0.2)\n",
            "Collecting i2 (from config2py->py2store->babe<0.0.8,>=0.0.7->sae_lens)\n",
            "  Downloading i2-0.1.46-py3-none-any.whl.metadata (2.1 kB)\n",
            "Downloading sae_lens-5.9.1-py3-none-any.whl (131 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.3/131.3 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyzmq-26.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (920 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m920.1/920.1 kB\u001b[0m \u001b[31m66.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-2.21.0-py3-none-any.whl (527 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m527.3/527.3 kB\u001b[0m \u001b[31m49.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformer_lens-2.15.0-py3-none-any.whl (189 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.2/189.2 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m128.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m98.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m62.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m110.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading automated_interpretability-0.0.8-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.3/66.3 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading babe-0.0.7-py3-none-any.whl (6.9 kB)\n",
            "Downloading beartype-0.14.1-py3-none-any.whl (739 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m739.7/739.7 kB\u001b[0m \u001b[31m62.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading better_abc-0.0.3-py3-none-any.whl (3.5 kB)\n",
            "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fancy_einsum-0.0.3-py3-none-any.whl (6.2 kB)\n",
            "Downloading fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.6/177.6 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jaxtyping-0.3.1-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.3/55.3 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading plotly_express-0.4.1-py2.py3-none-any.whl (2.9 kB)\n",
            "Downloading pytest_profiling-1.8.1-py3-none-any.whl (9.9 kB)\n",
            "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading safetensors-0.4.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m435.0/435.0 kB\u001b[0m \u001b[31m47.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typer-0.12.5-py3-none-any.whl (47 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.3/47.3 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading zstandard-0.22.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m109.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading blobfile-2.1.1-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.7/73.7 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading boostedblob-0.15.6-py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.2/59.2 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m74.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wadler_lindig-0.1.5-py3-none-any.whl (20 kB)\n",
            "Downloading gprof2dot-2025.4.14-py3-none-any.whl (37 kB)\n",
            "Downloading graze-0.1.29-py3-none-any.whl (19 kB)\n",
            "Downloading lxml-4.9.4-cp311-cp311-manylinux_2_28_x86_64.whl (7.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m110.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycryptodomex-3.22.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m101.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m124.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading config2py-0.1.37-py3-none-any.whl (32 kB)\n",
            "Downloading dol-0.3.16-py3-none-any.whl (254 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.7/254.7 kB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading i2-0.1.46-py3-none-any.whl (202 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m202.8/202.8 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: transformers-stream-generator, py2store\n",
            "  Building wheel for transformers-stream-generator (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers-stream-generator: filename=transformers_stream_generator-0.0.5-py3-none-any.whl size=12426 sha256=aa0c63fa4a7ec1d91257e9fbb1d32f2b03dcbbebf2c98a97348c91fe41db9868\n",
            "  Stored in directory: /root/.cache/pip/wheels/23/e8/f0/b3c58c12d1ffe60bcc8c7d121115f26b2c1878653edfca48db\n",
            "  Building wheel for py2store (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py2store: filename=py2store-0.1.20-py3-none-any.whl size=118411 sha256=f0887ab581e725163fc4c9ba56088fbeec9e47211bcfab6456e6a64053a4d41d\n",
            "  Stored in directory: /root/.cache/pip/wheels/b6/c0/a9/8ba28129562790a3ba62e4de4241dac5474df73d0e8a64e27a\n",
            "Successfully built transformers-stream-generator py2store\n",
            "Installing collected packages: i2, dol, better-abc, zstandard, xxhash, wadler-lindig, uvloop, safetensors, pyzmq, python-dotenv, pycryptodomex, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, lxml, gprof2dot, fsspec, fancy-einsum, dill, config2py, beartype, tiktoken, pytest-profiling, py2store, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, jaxtyping, httpx, graze, blobfile, typer, nvidia-cusolver-cu12, boostedblob, babe, plotly-express, datasets, automated-interpretability, transformers-stream-generator, transformer_lens, sae_lens\n",
            "  Attempting uninstall: zstandard\n",
            "    Found existing installation: zstandard 0.23.0\n",
            "    Uninstalling zstandard-0.23.0:\n",
            "      Successfully uninstalled zstandard-0.23.0\n",
            "  Attempting uninstall: safetensors\n",
            "    Found existing installation: safetensors 0.5.3\n",
            "    Uninstalling safetensors-0.5.3:\n",
            "      Successfully uninstalled safetensors-0.5.3\n",
            "  Attempting uninstall: pyzmq\n",
            "    Found existing installation: pyzmq 24.0.1\n",
            "    Uninstalling pyzmq-24.0.1:\n",
            "      Successfully uninstalled pyzmq-24.0.1\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: lxml\n",
            "    Found existing installation: lxml 5.3.2\n",
            "    Uninstalling lxml-5.3.2:\n",
            "      Successfully uninstalled lxml-5.3.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: httpx\n",
            "    Found existing installation: httpx 0.28.1\n",
            "    Uninstalling httpx-0.28.1:\n",
            "      Successfully uninstalled httpx-0.28.1\n",
            "  Attempting uninstall: typer\n",
            "    Found existing installation: typer 0.15.2\n",
            "    Uninstalling typer-0.15.2:\n",
            "      Successfully uninstalled typer-0.15.2\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.6.1 which is incompatible.\n",
            "langsmith 0.3.31 requires zstandard<0.24.0,>=0.23.0, but you have zstandard 0.22.0 which is incompatible.\n",
            "google-genai 1.10.0 requires httpx<1.0.0,>=0.28.1, but you have httpx 0.27.2 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed automated-interpretability-0.0.8 babe-0.0.7 beartype-0.14.1 better-abc-0.0.3 blobfile-2.1.1 boostedblob-0.15.6 config2py-0.1.37 datasets-2.21.0 dill-0.3.8 dol-0.3.16 fancy-einsum-0.0.3 fsspec-2024.6.1 gprof2dot-2025.4.14 graze-0.1.29 httpx-0.27.2 i2-0.1.46 jaxtyping-0.3.1 lxml-4.9.4 multiprocess-0.70.16 numpy-1.26.4 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 plotly-express-0.4.1 py2store-0.1.20 pycryptodomex-3.22.0 pytest-profiling-1.8.1 python-dotenv-1.1.0 pyzmq-26.0.0 sae_lens-5.9.1 safetensors-0.4.5 tiktoken-0.9.0 transformer_lens-2.15.0 transformers-stream-generator-0.0.5 typer-0.12.5 uvloop-0.21.0 wadler-lindig-0.1.5 xxhash-3.5.0 zstandard-0.22.0\n"
          ]
        }
      ],
      "source": [
        "# Install required libraries\n",
        "!pip install transformers datasets sae_lens transformer_lens torch numpy matplotlib seaborn wordcloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xcfwABo-j1Q-",
        "outputId": "ca7f7452-447b-4e0e-eaa5-251b5b65eae2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "from datasets import load_dataset\n",
        "from torch.cuda.amp import autocast\n",
        "from sae_lens import SAE\n",
        "from transformer_lens import HookedTransformer\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud\n",
        "from tqdm import tqdm\n",
        "import torch.nn.functional as F\n",
        "import gc\n",
        "\n",
        "\n",
        "# Set device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101,
          "referenced_widgets": [
            "b19d166b006b4e588e867d05de9d8b6b",
            "6a6741153c224fa2aa35420e74c4b854",
            "40317fbe60fb41c3987f168c5e281658",
            "a7d7e11b3b0444bf9477a090ea25960c",
            "a49ef40c042c4939947e30491efc50ce",
            "44525ca8cd5042c8aa45134b86c43983",
            "8b8e68a51cfe4245a15367ef7cd4c174",
            "647f4173586047a8bd3e16513f492884",
            "731921dfe67940adae9ee8342ee48118",
            "3033b5c9b87a4fe3b0d8463e5eed33f6",
            "89c390357e624ddbac9600f3c69086eb"
          ]
        },
        "id": "JLT6tAIxj738",
        "outputId": "6f528704-04ab-4a9f-fee6-5e41f7b81a80"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b19d166b006b4e588e867d05de9d8b6b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:With reduced precision, it is advised to use `from_pretrained_no_processing` instead of `from_pretrained`.\n",
            "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded pretrained model meta-llama/Meta-Llama-3-8B-Instruct into HookedTransformer\n"
          ]
        }
      ],
      "source": [
        "\n",
        "model_name = 'meta-llama/Meta-Llama-3-8B-Instruct'\n",
        "\n",
        "# Convert to HookedTransformer for hooking capabilities\n",
        "hooked_model = HookedTransformer.from_pretrained(\n",
        "    model_name = model_name,\n",
        "    device=device,\n",
        "    dtype=torch.float16,\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXjMHyVhkon8",
        "outputId": "c78dc503-e8bc-4468-9660-6b1d17122466"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Datasets loaded and cleaned.\n"
          ]
        }
      ],
      "source": [
        "# Load WikiText-2 dataset\n",
        "wikitext2 = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
        "\n",
        "# Load Harry Potter Book 1 \n",
        "\n",
        "hp_book_path = \"/content/drive/MyDrive/Unlearning/Harry_Potter_Book1.txt\"  \n",
        "hp_book = load_dataset(\"text\", data_files={\"train\": hp_book_path})[\"train\"]\n",
        "\n",
        "# Function to clean WikiText-2 by removing Harry Potter mentions\n",
        "def clean_wikitext(sample):\n",
        "    text = sample[\"text\"].lower()\n",
        "    if \"harry potter\" in text:\n",
        "        return {\"text\": \"\"}\n",
        "    return sample\n",
        "\n",
        "# Clean the dataset and filter out empty entries\n",
        "wikitext2_cleaned = wikitext2.map(clean_wikitext)\n",
        "wikitext2_cleaned = wikitext2_cleaned.filter(lambda x: x[\"text\"].strip() != \"\")\n",
        "\n",
        "# Take smaller subsets for efficiency (adjust as needed)\n",
        "wikitext2_small = wikitext2.select(range(1000))\n",
        "wikitext2_cleaned_small = wikitext2_cleaned.select(range(1000))\n",
        "hp_dataset_small = hp_book.select(range(1000))\n",
        "\n",
        "print(\"Datasets loaded and cleaned.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mhwx8Sc09APN"
      },
      "outputs": [],
      "source": [
        "def compute_hidden_states(texts, model, layer, max_tokens=512):\n",
        "    hidden_states = []\n",
        "    for text in tqdm(texts, desc=f\"Computing hidden states for layer {layer}\"):\n",
        "        tokens = model.to_tokens(text)[:, :max_tokens].to(device)\n",
        "        with torch.no_grad(), autocast():\n",
        "            _, cache = model.run_with_cache(\n",
        "                tokens,\n",
        "                names_filter=[f\"blocks.{layer}.hook_resid_post\"]\n",
        "            )\n",
        "        hidden_state = cache[f\"blocks.{layer}.hook_resid_post\"].squeeze(0).cpu()  # Move to CPU\n",
        "        del cache  # Free memory immediately\n",
        "        hidden_states.append(hidden_state)\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "    return hidden_states\n",
        "\n",
        "# Function to load SAE\n",
        "def load_sae(layer, device, release=\"llama-3-8b-it-res-jh\"):\n",
        "    print(f\"Loading SAE for layer {layer}\")\n",
        "    try:\n",
        "        sae, _, _ = SAE.from_pretrained(\n",
        "            release=release,\n",
        "            sae_id=f\"blocks.{layer}.hook_resid_post\",\n",
        "            device=device\n",
        "        )\n",
        "        print(f\"Successfully loaded SAE for layer {layer}\")\n",
        "        return sae\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading SAE for layer {layer}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Function to compute activations from hidden states\n",
        "def compute_activations_from_hidden_states(hidden_states, sae, device):\n",
        "    total_sum = torch.zeros(sae.cfg.d_sae, device=device, dtype=torch.float16)\n",
        "    total_tokens = 0\n",
        "    for hidden_state in tqdm(hidden_states, desc=\"Computing activations\"):\n",
        "        hidden_state = hidden_state.to(device)\n",
        "        with torch.no_grad(), autocast():\n",
        "            feature_acts = sae.encode(hidden_state)  # Shape: (num_tokens, d_sae)\n",
        "        total_sum += feature_acts.sum(dim=0)\n",
        "        total_tokens += feature_acts.size(0)\n",
        "        del hidden_state, feature_acts  # Free memory\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "    if total_tokens > 0:\n",
        "        return total_sum / total_tokens\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cMtHkzmcej_T",
        "outputId": "9d0434d2-4b34-47f5-d774-ef0d55b3b545"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rComputing hidden states for layer 25:   0%|          | 0/50 [00:00<?, ?it/s]<ipython-input-7-cdb63574270c>:5: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.no_grad(), autocast():\n",
            "Computing hidden states for layer 25: 100%|██████████| 50/50 [00:22<00:00,  2.23it/s]\n",
            "Computing hidden states for layer 25: 100%|██████████| 50/50 [00:22<00:00,  2.24it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model deleted, GPU memory freed.\n",
            "Loading SAE for layer 25\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sae_lens/sae.py:654: UserWarning: norm_scaling_factor not found for llama-3-8b-it-res-jh and blocks.25.hook_resid_post, but normalize_activations is 'expected_average_only_in'. Skipping normalization folding.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully loaded SAE for layer 25\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rComputing activations:   0%|          | 0/50 [00:00<?, ?it/s]<ipython-input-7-cdb63574270c>:38: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.no_grad(), autocast():\n",
            "Computing activations: 100%|██████████| 50/50 [00:16<00:00,  2.96it/s]\n",
            "Computing activations: 100%|██████████| 50/50 [00:17<00:00,  2.93it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layer 25: Identified top features: [63905, 7876, 7754, 30919, 3643]\n",
            "Top features identified: {25: [63905, 7876, 7754, 30919, 3643]}\n"
          ]
        }
      ],
      "source": [
        "# Main computation\n",
        "desired_layers = [25]\n",
        "top_features = {}\n",
        "hp_texts = [sample[\"text\"] for sample in hp_dataset_small.select(range(50))]\n",
        "general_texts = [sample[\"text\"] for sample in wikitext2_cleaned_small.select(range(50))]\n",
        "\n",
        "for layer in desired_layers:\n",
        "    # Step 1: Compute hidden states\n",
        "    hp_hidden_states = compute_hidden_states(hp_texts, hooked_model, layer)\n",
        "    general_hidden_states = compute_hidden_states(general_texts, hooked_model, layer)\n",
        "\n",
        "    # Step 2: Free GPU memory by deleting the model\n",
        "    del hooked_model\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    print(\"Model deleted, GPU memory freed.\")\n",
        "\n",
        "    # Step 3: Load SAE\n",
        "    sae = load_sae(layer, device)\n",
        "    if sae is None:\n",
        "        print(f\"Skipping layer {layer} due to SAE loading failure\")\n",
        "        continue\n",
        "\n",
        "    # Step 4: Compute activations\n",
        "    hp_activations = compute_activations_from_hidden_states(hp_hidden_states, sae, device)\n",
        "    general_activations = compute_activations_from_hidden_states(general_hidden_states, sae, device)\n",
        "\n",
        "    # Step 5: Identify top features\n",
        "    if hp_activations is not None and general_activations is not None:\n",
        "        diff = hp_activations - general_activations\n",
        "        top_indices = torch.topk(diff, k=5).indices.tolist()\n",
        "        top_features[layer] = top_indices\n",
        "        print(f\"Layer {layer}: Identified top features: {top_indices}\")\n",
        "    else:\n",
        "        print(f\"Layer {layer}: Failed to compute activations.\")\n",
        "\n",
        "    # Step 6: Clean up\n",
        "    del sae, hp_hidden_states, general_hidden_states\n",
        "    if 'hp_activations' in locals():\n",
        "        del hp_activations\n",
        "    if 'general_activations' in locals():\n",
        "        del general_activations\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "# Output results\n",
        "print(\"Top features identified:\", top_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9lgKMM6FfyL",
        "outputId": "d90917fa-5ab3-4a35-e4fa-0b09302c3464"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top features saved to 'top_features_llama.json'.\n"
          ]
        }
      ],
      "source": [
        "# Save top features to a file\n",
        "with open(\"/content/drive/MyDrive/Unlearning/top_features_llama_8b_instruct.json\", \"w\") as f:\n",
        "    json.dump(top_features, f)\n",
        "print(\"Top features saved to 'top_features_llama.json'.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9_HdM26FjbJ",
        "outputId": "d14788e0-f959-4113-8df0-6b7150e25bb5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Computing perplexity: 100%|██████████| 100/100 [00:08<00:00, 12.09it/s]\n",
            "Computing perplexity: 100%|██████████| 100/100 [00:05<00:00, 18.63it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No ablation - WikiText-2: 20.48, Harry Potter: 10.65\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Function to compute perplexity\n",
        "def compute_perplexity(dataset, model, device, hooks=None, max_samples=100):\n",
        "    total_loss = 0.0\n",
        "    total_tokens = 0\n",
        "    hooks = hooks if hooks else []\n",
        "    for sample in tqdm(dataset.select(range(max_samples)), desc=\"Computing perplexity\"):\n",
        "        text = sample[\"text\"].strip()\n",
        "        if not text:\n",
        "            continue\n",
        "        tokens = model.to_tokens(text).to(device)\n",
        "        with model.hooks(fwd_hooks=hooks):\n",
        "            with torch.no_grad():\n",
        "                logits = model(tokens)\n",
        "        shift_logits = logits[:, :-1, :]\n",
        "        shift_labels = tokens[:, 1:]\n",
        "        loss = F.cross_entropy(\n",
        "            shift_logits.reshape(-1, shift_logits.size(-1)),\n",
        "            shift_labels.reshape(-1),\n",
        "            reduction=\"sum\"\n",
        "        )\n",
        "        total_loss += loss.item()\n",
        "        total_tokens += shift_labels.numel()\n",
        "    avg_loss = total_loss / total_tokens if total_tokens > 0 else float('inf')\n",
        "    return torch.exp(torch.tensor(avg_loss)).item()\n",
        "\n",
        "# Compute perplexity without ablation\n",
        "ppl_no_ablation_wikitext = compute_perplexity(wikitext2_small, hooked_model, device, max_samples=100)\n",
        "ppl_no_ablation_hp = compute_perplexity(hp_dataset_small, hooked_model, device, max_samples=100)\n",
        "print(f\"No ablation - WikiText-2: {ppl_no_ablation_wikitext:.2f}, Harry Potter: {ppl_no_ablation_hp:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8CSTvqtQwxz",
        "outputId": "5a35e441-5e50-4244-c665-92aef01b25c5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6nYYseExMGXp",
        "outputId": "bcba8494-4f23-4f89-8a4e-62dd38de4b05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading SAE for layer 25\n",
            "Successfully loaded SAE for layer 25\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Computing perplexity: 100%|██████████| 100/100 [00:08<00:00, 11.43it/s]\n",
            "Computing perplexity: 100%|██████████| 100/100 [00:05<00:00, 17.49it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layer 25 - With ablation - WikiText-2: 63.32, Harry Potter: 60.64\n"
          ]
        }
      ],
      "source": [
        "layers_with_sae = list(top_features.keys())\n",
        "\n",
        "# Compute perplexity with ablation for each layer\n",
        "for layer in layers_with_sae:\n",
        "    sae = load_sae(layer, device)\n",
        "    if sae is not None:\n",
        "        top_feats = top_features[layer]\n",
        "        top_feats = top_feats[:5]\n",
        "        # Define ablation hook for this layer\n",
        "        def ablate_hook(hidden_state, hook):\n",
        "            if hook.name == f\"blocks.{layer}.hook_resid_pre\":\n",
        "                batch, seq_len, d_model = hidden_state.shape\n",
        "                hidden_state_flat = hidden_state.view(batch * seq_len, d_model)\n",
        "                feature_acts = sae.encode(hidden_state_flat)\n",
        "                selected_features = torch.tensor(top_feats, device=device)\n",
        "                feature_acts[:, selected_features] = 0  # Ablate top features\n",
        "                modified_hidden_state_flat = sae.decode(feature_acts)\n",
        "                return modified_hidden_state_flat.view(batch, seq_len, d_model)\n",
        "            return hidden_state\n",
        "\n",
        "        # Compute perplexity with ablation\n",
        "        ppl_with_ablation_wikitext = compute_perplexity(\n",
        "            wikitext2_small, hooked_model, device,\n",
        "            hooks=[(f\"blocks.{layer}.hook_resid_pre\", ablate_hook)],\n",
        "            max_samples=100\n",
        "        )\n",
        "        ppl_with_ablation_hp = compute_perplexity(\n",
        "            hp_dataset_small, hooked_model, device,\n",
        "            hooks=[(f\"blocks.{layer}.hook_resid_pre\", ablate_hook)],\n",
        "            max_samples=100\n",
        "        )\n",
        "        print(f\"Layer {layer} - With ablation - WikiText-2: {ppl_with_ablation_wikitext:.2f}, Harry Potter: {ppl_with_ablation_hp:.2f}\")\n",
        "\n",
        "        # Clean up\n",
        "        del sae\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_2hl36ANpYS",
        "outputId": "cdc3c0b4-9158-46cd-c3c0-11edeb897b80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Text Generation: No Ablation ===\n",
            "Prompt: Who is Harry Potter?\n",
            "Generated: <|begin_of_text|>Who is Harry Potter? Harry Potter is a young wizard who is the main character in a series of fantasy novels by J.K. Rowling. The books follow Harry's adventures at Hogwarts School of Witchcraft and Wizardry, where he makes friends and battles against the dark wizard,\n",
            "\n",
            "Prompt: Who is Ron Weasley\n",
            "Generated: <|begin_of_text|>Who is Ron Weasley?\n",
            "Ron Weasley is a fictional character in the Harry Potter book series by J.K. Rowling. He is one of the best friends of the main protagonist, Harry Potter, and a member of Gryffindor House at Hogwarts School of Witchcraft\n",
            "\n",
            "Prompt: Tell me about Harry Potter's adventures at Hogwarts.\n",
            "Generated: <|begin_of_text|>Tell me about Harry Potter's adventures at Hogwarts. Did you enjoy reading the books or watching the movies?\n",
            "I loved reading the Harry Potter books! The series is incredibly magical and immersive, and J.K. Rowling's writing is phenomenal. The books are full of richly detailed world-building, memorable characters\n",
            "\n",
            "Prompt: What is the significance of the Sorting Hat in Hogwarts?\n",
            "Generated: <|begin_of_text|>What is the significance of the Sorting Hat in Hogwarts? [duplicate]\n",
            "The Sorting Hat is a magical artifact that is used in the first year of a student's education at Hogwarts School of Witchcraft and Wizardry to sort them into one of four houses: Gryffindor, Slytherin, Raven\n",
            "\n",
            "Prompt: Explain the history of the Roman Empire.\n",
            "Generated: <|begin_of_text|>Explain the history of the Roman Empire. The Roman Empire was a vast and powerful state that existed from 27 BC to 476 AD. It was founded by Augustus Caesar, who was the grandnephew and adopted son of Julius Caesar. Augustus established the Principate, a\n",
            "\n",
            "Prompt: What are the benefits of a healthy diet?\n",
            "Generated: <|begin_of_text|>What are the benefits of a healthy diet? A healthy diet provides numerous benefits for the body and brain. Some of the most significant benefits include:\n",
            "1. Weight management: A healthy diet helps to maintain a healthy weight, which can reduce the risk of chronic diseases such as diabetes, heart disease,\n",
            "\n",
            "Prompt: Describe a typical day in a modern city.\n",
            "Generated: <|begin_of_text|>Describe a typical day in a modern city. What are some of the common features of urban life?\n",
            "A typical day in a modern city is always bustling with activity. The day begins early, with the sound of buses and cars hitting the roads, and the smell of freshly brewed coffee wafting\n",
            "\n",
            "Loading SAE for layer 25\n",
            "Successfully loaded SAE for layer 25\n",
            "\n",
            "=== Text Generation: Ablation for Layer 25 ===\n",
            "Prompt: Who is Harry Potter?\n",
            "Generated: <|begin_of_text|>Who is Harry Potter? I the in the hands of the little it. The new.\n",
            "About 50,000 people were in attendance at the event, which was held at the WizCon, a convention of the Harry Potter fandom. ...The BBC has been making it\n",
            "\n",
            "Prompt: Who is Ron Weasley\n",
            "Generated: <|begin_of_text|>Who is Ron Weasley, and why are you talking about him?\n",
            "Ron, short for Ronald Desmond, is a main character in the Harry Potter series by J.K. Rowling. He is a half-brother of Harry, and the oldest son of the family of the\n",
            "\n",
            "Prompt: Tell me about Harry Potter's adventures at Hogwarts.\n",
            "Generated: <|begin_of_text|>Tell me about Harry Potter's adventures at Hogwarts. In the same. It's unclear if the author intended to have the audience think in so much to the world the characters live in. The story is set in the world of Hogwarts, a magical school, and follows the adventures of the main character,\n",
            "\n",
            "Prompt: What is the significance of the Sorting Hat in Hogwarts?\n",
            "Generated: <|begin_of_text|>What is the significance of the Sorting Hat in Hogwarts? 5 answers\n",
            "What is the...\n",
            "\n",
            "#### 5 answers▍\n",
            "\n",
            "1. Classification\n",
            "\n",
            "The Sorting Hat is a key component of the magical world of Harry Potter. It is a magical hat that is used to divide students into one of three groups:\n",
            "\n",
            "Prompt: Explain the history of the Roman Empire.\n",
            "Generated: <|begin_of_text|>Explain the history of the Roman Empire. How did it arise, and how did it decline?\n",
            "The Roman Empire, which was established in 27 BCE, is one of the most significant and influential civilizations in history. It was founded by Augustus, a Roman dictator, and it lasted for\n",
            "\n",
            "Prompt: What are the benefits of a healthy diet?\n",
            "Generated: <|begin_of_text|>What are the benefits of a healthy diet? A well-balanced diet can provide several health benefits, including:\n",
            "\n",
            "1. Improved overall health: A well-balanced diet can provide the body with the nutrients and nutrients it needs to function properly, which can lead to improved overall health.\n",
            "\n",
            "2. Improved\n",
            "\n",
            "Prompt: Describe a typical day in a modern city.\n",
            "Generated: <|begin_of_text|>Describe a typical day in a modern city. Describe the noise, the sights, the smells, the feeling of being there.\n",
            "\n",
            "At 6:00 AM, the sun begins to rise, casting a golden glow over the city. The sky is filled with the faint scent of freshly brewed coffee and\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#  sample prompts for text generation\n",
        "hp_prompts = [\n",
        "    \"Who is Harry Potter?\",\n",
        "    \"Who is Ron Weasley\",\n",
        "    \"Tell me about Harry Potter's adventures at Hogwarts.\",\n",
        "    \"What is the significance of the Sorting Hat in Hogwarts?\"\n",
        "]\n",
        "general_prompts = [\n",
        "    \"Explain the history of the Roman Empire.\",\n",
        "    \"What are the benefits of a healthy diet?\",\n",
        "    \"Describe a typical day in a modern city.\"\n",
        "]\n",
        "\n",
        "# Function to generate text\n",
        "def generate_text(model, prompt, hooks=None, max_new_tokens=50, temperature=0.7):\n",
        "    tokens = model.to_tokens(prompt).to(device)\n",
        "    hooks = hooks if hooks else []\n",
        "    with model.hooks(fwd_hooks=hooks):\n",
        "        generated_tokens = model.generate(\n",
        "            tokens,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=temperature,\n",
        "            verbose=False\n",
        "        )\n",
        "    return model.to_string(generated_tokens)[0]\n",
        "\n",
        "# Generate text without ablation\n",
        "print(\"\\n=== Text Generation: No Ablation ===\")\n",
        "for prompt in hp_prompts + general_prompts:\n",
        "    generated = generate_text(hooked_model, prompt, hooks=None)\n",
        "    print(f\"Prompt: {prompt}\\nGenerated: {generated}\\n\")\n",
        "\n",
        "# Generate text with ablation for each layer\n",
        "for layer in layers_with_sae:\n",
        "    sae = load_sae(layer, device)\n",
        "    if sae is not None:\n",
        "        top_feats = top_features[layer]\n",
        "        top_feats = top_feats[:5]\n",
        "        # Define ablation hook for this layer\n",
        "        def ablate_hook(hidden_state, hook):\n",
        "            if hook.name == f\"blocks.{layer}.hook_resid_pre\":\n",
        "                batch, seq_len, d_model = hidden_state.shape\n",
        "                hidden_state_flat = hidden_state.view(batch * seq_len, d_model)\n",
        "                feature_acts = sae.encode(hidden_state_flat)\n",
        "                selected_features = torch.tensor(top_feats, device=device)\n",
        "                feature_acts[:, selected_features] = 0  # Ablate top features\n",
        "                modified_hidden_state_flat = sae.decode(feature_acts)\n",
        "                return modified_hidden_state_flat.view(batch, seq_len, d_model)\n",
        "            return hidden_state\n",
        "\n",
        "        print(f\"\\n=== Text Generation: Ablation for Layer {layer} ===\")\n",
        "        for prompt in hp_prompts + general_prompts:\n",
        "            generated = generate_text(\n",
        "                hooked_model, prompt,\n",
        "                hooks=[(f\"blocks.{layer}.hook_resid_pre\", ablate_hook)]\n",
        "            )\n",
        "            print(f\"Prompt: {prompt}\\nGenerated: {generated}\\n\")\n",
        "\n",
        "        # Clean up\n",
        "        del sae\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pthVBV1UNrK1"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3033b5c9b87a4fe3b0d8463e5eed33f6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40317fbe60fb41c3987f168c5e281658": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_647f4173586047a8bd3e16513f492884",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_731921dfe67940adae9ee8342ee48118",
            "value": 4
          }
        },
        "44525ca8cd5042c8aa45134b86c43983": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "647f4173586047a8bd3e16513f492884": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6a6741153c224fa2aa35420e74c4b854": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_44525ca8cd5042c8aa45134b86c43983",
            "placeholder": "​",
            "style": "IPY_MODEL_8b8e68a51cfe4245a15367ef7cd4c174",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "731921dfe67940adae9ee8342ee48118": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "89c390357e624ddbac9600f3c69086eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8b8e68a51cfe4245a15367ef7cd4c174": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a49ef40c042c4939947e30491efc50ce": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a7d7e11b3b0444bf9477a090ea25960c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3033b5c9b87a4fe3b0d8463e5eed33f6",
            "placeholder": "​",
            "style": "IPY_MODEL_89c390357e624ddbac9600f3c69086eb",
            "value": " 4/4 [00:03&lt;00:00,  1.37it/s]"
          }
        },
        "b19d166b006b4e588e867d05de9d8b6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6a6741153c224fa2aa35420e74c4b854",
              "IPY_MODEL_40317fbe60fb41c3987f168c5e281658",
              "IPY_MODEL_a7d7e11b3b0444bf9477a090ea25960c"
            ],
            "layout": "IPY_MODEL_a49ef40c042c4939947e30491efc50ce"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
